<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Implementation | Crowd Dynamics at Harvard University</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Implementation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Repository for CS 205 Final Project, Spring 2019" />
<meta property="og:description" content="Repository for CS 205 Final Project, Spring 2019" />
<link rel="canonical" href="http://localhost:4000/implementation.html" />
<meta property="og:url" content="http://localhost:4000/implementation.html" />
<meta property="og:site_name" content="Crowd Dynamics at Harvard University" />
<script type="application/ld+json">
{"url":"http://localhost:4000/implementation.html","headline":"Implementation","description":"Repository for CS 205 Final Project, Spring 2019","@type":"WebPage","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Crowd Dynamics at Harvard University" /></head><body><header class="site-header" style="display: flex">
        <img class="h_logo" src="/images/seas.png" >
    <div class="wrapper"><a class="site-title" rel="author" href="/">Crowd Dynamics at Harvard University</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger"><a class="page-link" href="/">Overview</a><a class="page-link" href="/implementation.html">Implementation</a><a class="page-link" href="/requirements.html">Requirements</a><a class="page-link" href="/tuning.html">Tuning</a><a class="page-link" href="/performance.html">Performance</a><a class="page-link" href="/visualizations.html">Visualizations</a></div>
        </nav></div>
        <img class="h_logo" src="/images/iacs.png" >
  </header>
  <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Implementation</h1>
  </header>

  <div class="post-content">
    <p>Here we describe the infrastructure, technologies, and platforms we used
throughout the entire pipeline.</p>

<h2 id="data-pipeline">Data Pipeline</h2>

<p>We mainly use distributed Tensorflow for the machine learning inference and
Spark for the following data analytics.
Our pipeline looks like this:</p>

<p><img src="/images/pipeline.png" alt="implementation" /></p>

<p>Our project can be split up into multiple stages.</p>

<h3 id="video-processing-and-object-detection">Video Processing and Object Detection</h3>

<p>First, we read in the historical video records of the Science Center Plaza and turn our collected video into 1 hour chunks.</p>

<p>We then use a TensorFlow implementation of an object detection model to generate bounding boxes.
In particular, we use a deep convolutional neural net using the Faster-RCNN architecture [1]
for detecting people, bicycles, cars, and trucks. We used a pretrained model that was trained on Microsoft COCO [2], 
a dataset of common objects in context. The dataset is composed of a large number of images with a total of 91 unique
objects with labels; however, we only care detecting pedestrians, so we only focus on detecting one class (person). 
For each frame, we end up generating a timestamp, bounding boxes, and scores for confidence of detection in each of those 
bounding boxes. We save this output data to Spark dataframes, where each row contains the output generated from one frame, which are then used when we compute our analytics.</p>

<p>We run this stage of our computation on a AWS EC2 instance with GPU, in particular a p3.8xlarge instance, which has 4 GPUs
with a Deep Learning AMI (Ubuntu) Version 22.0.</p>

<h3 id="analytics">Analytics</h3>

<p>To perform our analytics, we apply user defined function transformations to the columns of the Spark dataframes from the 
object detection step. These analytics include locations of people, number of people, group size, and velocities. After 
computing analytics on the historical data, we also aggregate over short windows of time (ex. 10 minutes). After all analytics
computations have been completed, we write our results to a Pandas dataframe to be used in the visualization code.</p>

<p>We run this stage of our computation on a AWS EMR cluster with Spark. In particular, we use emr-5.23.0, which has
Spark 2.4.0 on Hadoop 2.8.5 YARN with Ganglia 3.7.2 and Zeppelin 0.8.1, and have 8 m4.xlarge instances in our cluster.</p>

<p>We now provide some more details on how our analytics transformation were done. Locations of people and number of people are both fairly straightforward, to calculate, with location computed by finding the centers of the input bounding boxes and
number of people found by just counting the number of bounding boxes. Group size and velocity computations are a bit more involved, however, and are described in detail below.</p>

<h4 id="group-size">Group Size</h4>

<p>TODO: Add details of how this is done.</p>

<h4 id="velocity">Velocity</h4>

<p>In order to compute the average velocity in each frame, we must consider the detected objects in the next frame.
For person A in frame <script type="math/tex">i</script>, we need to identify the location of person A in frame <script type="math/tex">i+1</script>, and then approximate the velocity in frame i by dividing the distance traveled (between person A’s centers) by the time, which is the inverse of the fps (frames per second).</p>

<p>So, first, we must link people between frames.
Since people are constantly entering and leaving the frame, the number of objects per frame ($n_i$) may not be consistent between successive frames.
Therefore, for each frame, we can compute at most <script type="math/tex">\text{min}(n_i, n_{i+1})</script> velocities.
We compute pairwise distances between all objects in different frames, sort the pairs in ascending order of distance, and then greedily label each unassigned object in frame <script type="math/tex">i</script> to the closest unassigned object in frame <script type="math/tex">i+1</script>.
We account for the edge case of a person leaving one end of the frame and another person entering on the opposite side of the next frame by only linking people whose distance is within a predefined threshold of 30% of the frame width or length.
In particular, we compute the Euclidean distance between the centers (where the two dimensions of the centers are in <script type="math/tex">[0, 1]</script> as proportions of the frame dimension length), and then do not link objects if the distance exceeds <script type="math/tex">0.3</script>.</p>

<p>Here is an example image, where the first frame (A) has 6 detected people, and the second frame (B) has 3 detected people. The green lines denote the link, and the velocity is computed as the length of the green line times the fps.</p>

<p align="center"> 
<img src="images/linking.png" />
</p>

<h3 id="visualizations">Visualizations</h3>
<p>We take the augmented Spark dataframe from our analytics step and convert it back into a Pandas dataframe for
visualization. With this dataframe, create visualizations using the Bokeh library for interactive graphs,
and CV2 with matplotlib to draw the bounding boxes on the original video.</p>

<p>TODO: Add a more detail.</p>

<h2 id="citations">Citations</h2>

<ol>
  <li>
    <p>Ren, Shaoqing et al. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” IEEE Transactions on Pattern Analysis and Machine Intelligence 39.6 (2017): 1137–1149. Crossref. Web.</p>
  </li>
  <li>
    <p>Lin, Tsung-Yi et al. “Microsoft COCO: Common Objects in Context.” Lecture Notes in Computer Science (2014): 740–755. Crossref. Web.</p>
  </li>
</ol>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Crowd Dynamics at Harvard University</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Crowd Dynamics at Harvard University</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Repository for CS 205 Final Project, Spring 2019</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
